{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we load the dataset and drop some unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"train.parquet\")\n",
    "df.drop(columns=[\"ID\", \"efs_time\"], axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df[\"ethnicity\"] == \"Not Hispanic or Latino\") & (df[\"efs\"] == 1)].shape)\n",
    "print(df[(df[\"ethnicity\"] == \"Not Hispanic or Latino\") & (df[\"efs\"] == 0)].shape)\n",
    "print(df[(df[\"ethnicity\"] == \"Hispanic or Latino\") & (df[\"efs\"] == 1)].shape)\n",
    "print(df[(df[\"ethnicity\"] == \"Hispanic or Latino\") & (df[\"efs\"] == 0)].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we do some fillna operations for the null values and use labelencoder for simple features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "\n",
    "TARGET = [\"efs\"]\n",
    "FEATURES = [c for c in df.columns if c not in TARGET]\n",
    "\n",
    "for c in FEATURES:\n",
    "    if df[c].dtype == \"object\" or df[c].dtype == \"category\":\n",
    "        cat_cols.append(c)\n",
    "    else:\n",
    "        num_cols.append(c)\n",
    "print(f\"In these features, there are {len(cat_cols)} CATEGORICAL FEATURES: {cat_cols}\")\n",
    "\n",
    "\n",
    "def update(df, cat_cols):\n",
    "    for c in cat_cols:\n",
    "        df[c] = df[c].astype(str).fillna(\"Unknown\").astype(\"category\")\n",
    "    for c in num_cols:\n",
    "        if df[c].dtype == \"float64\":\n",
    "            df[c] = df[c].fillna(0).astype(\"float32\")\n",
    "        if df[c].dtype == \"int64\":\n",
    "            df[c] = df[c].fillna(0).astype(\"int32\")\n",
    "    return df\n",
    "\n",
    "df[\"efs\"] = df[\"efs\"].astype(\"int32\")\n",
    "df = update(df, cat_cols)\n",
    "for col in cat_cols:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "df.sample(frac=1.0).reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we set the sensitive attribute to be \"ethnicity\" and split the dataset into df and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ethnicity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in df.columns if c not in TARGET]\n",
    "sensitive_attribute = features.index(\"ethnicity\")\n",
    "print(sensitive_attribute)\n",
    "\n",
    "target = [\"efs\"]\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy().ravel()\n",
    "y_test = y_test.to_numpy().ravel()\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we report the per-group prevalance for z=0 and z=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = (X_test[:, sensitive_attribute] == 1)\n",
    "\n",
    "prevalence_0 = np.mean(y_test[z_test == 0])\n",
    "prevalence_1 = np.mean(y_test[z_test == 1])\n",
    "\n",
    "prevalence_0, prevalence_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we train the basic model and report the demographic parity on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1, max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "z_test = (X_test[:, sensitive_attribute] == 1)\n",
    "\n",
    "positive_pred_z_0 = np.mean(y_pred[z_test == 0])\n",
    "positive_pred_z_1 = np.mean(y_pred[z_test == 1])\n",
    "\n",
    "parity = positive_pred_z_0 / positive_pred_z_1\n",
    "parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we report TPR_0, TPR_1, FPR_0, and FPR_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = (X_test[:, sensitive_attribute] == 1)\n",
    "\n",
    "tp_0 = np.sum((y_pred == 1) & (y_test == 1) & (z_test == 0))\n",
    "fn_0 = np.sum((y_pred == 0) & (y_test == 1) & (z_test == 0))\n",
    "fp_0 = np.sum((y_pred == 1) & (y_test == 0) & (z_test == 0))\n",
    "tn_0 = np.sum((y_pred == 0) & (y_test == 0) & (z_test == 0))\n",
    "\n",
    "tp_1 = np.sum((y_pred == 1) & (y_test == 1) & (z_test == 1))\n",
    "fn_1 = np.sum((y_pred == 0) & (y_test == 1) & (z_test == 1))\n",
    "fp_1 = np.sum((y_pred == 1) & (y_test == 0) & (z_test == 1))\n",
    "tn_1 = np.sum((y_pred == 0) & (y_test == 0) & (z_test == 1))\n",
    "\n",
    "TPR_0 = tp_0 / (tp_0 + fn_0)\n",
    "TPR_1 = tp_1 / (tp_1 + fn_1)\n",
    "\n",
    "FPR_0 = fp_0 / (fp_0 + tn_0)\n",
    "FPR_1 = fp_1 / (fp_1 + tn_1)\n",
    "\n",
    "TPR_0, TPR_1, FPR_0, FPR_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
