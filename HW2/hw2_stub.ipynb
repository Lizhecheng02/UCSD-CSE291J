{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c621447",
   "metadata": {},
   "source": [
    "## Homework 2: Intro to bias and fairness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3b9642c",
   "metadata": {},
   "source": [
    "Download the German Credit dataset: https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data\n",
    "(use the “numeric” version of the data)\n",
    "\n",
    "Implement a (logistic regression) classification pipeline using an 80/20 test split. Use a regularization value of C = 1.\n",
    "\n",
    "Treat the 20th feature (i.e., feat[19] in the numeric data, which is\n",
    "related to housing) as the “sensitive attribute” i.e., z=1 if the feature value is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f42dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "german_credit = np.loadtxt(\"german.data-numeric\")\n",
    "attrs = german_credit[:, :-1] \n",
    "labels = 2 - german_credit[:, -1]\n",
    "\n",
    "split_point = 800\n",
    "X_train, X_test = attrs[:split_point], attrs[split_point:]\n",
    "y_train, y_test = labels[:split_point], labels[split_point:]\n",
    "\n",
    "sensitive_attribute = 19\n",
    "\n",
    "model = LogisticRegression(C=1, max_iter=1000, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce91dbf0",
   "metadata": {},
   "source": [
    "1. Report the prevalence in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9305af18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.695"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevalence = np.mean(y_test)\n",
    "\n",
    "prevalence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4845af",
   "metadata": {},
   "source": [
    "2. Report the per-group prevalence for z=0 and z=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2be0627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7204968944099379, 0.5897435897435898)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_test = (X_test[:, sensitive_attribute] == 1)\n",
    "\n",
    "prevalence_0 = np.mean(y_test[(z_test == 0)])\n",
    "prevalence_1 = np.mean(y_test[(z_test == 1)])\n",
    "\n",
    "prevalence_0, prevalence_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca0bdadb",
   "metadata": {},
   "source": [
    "3. What is the demographic parity (expressed as a ratio between z=0 and z=1) for your classifier on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992bd2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2014906832298136"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "z_test = (X_test[:, sensitive_attribute] == 1)\n",
    "\n",
    "positive_pred_z_0 = np.mean(y_pred[z_test == 0])\n",
    "positive_pred_z_1 = np.mean(y_pred[z_test == 1])\n",
    "\n",
    "parity = positive_pred_z_0 / positive_pred_z_1\n",
    "\n",
    "parity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d94e8371",
   "metadata": {},
   "source": [
    "4. Report TPR_0, TPR_1, FPR_0, and FPR_1 (see \"equal opportunity\" slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa008798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8879310344827587, 0.8695652173913043, 0.4666666666666667, 0.3125)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_test = (X_test[:, sensitive_attribute] == 1)\n",
    "\n",
    "tp_0 = np.sum((y_pred == 1) & (y_test == 1) & (z_test == 0))\n",
    "fn_0 = np.sum((y_pred == 0) & (y_test == 1) & (z_test == 0))\n",
    "fp_0 = np.sum((y_pred == 1) & (y_test == 0) & (z_test == 0))\n",
    "tn_0 = np.sum((y_pred == 0) & (y_test == 0) & (z_test == 0))\n",
    "\n",
    "tp_1 = np.sum((y_pred == 1) & (y_test == 1) & (z_test == 1))\n",
    "fn_1 = np.sum((y_pred == 0) & (y_test == 1) & (z_test == 1))\n",
    "fp_1 = np.sum((y_pred == 1) & (y_test == 0) & (z_test == 1))\n",
    "tn_1 = np.sum((y_pred == 0) & (y_test == 0) & (z_test == 1))\n",
    "\n",
    "TPR_0 = tp_0 / (tp_0 + fn_0)\n",
    "TPR_1 = tp_1 / (tp_1 + fn_1)\n",
    "\n",
    "FPR_0 = fp_0 / (fp_0 + tn_0)\n",
    "FPR_1 = fp_1 / (fp_1 + tn_1)\n",
    "\n",
    "TPR_0, TPR_1, FPR_0, FPR_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a74d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code that I got 1.0 / 1.0 for the original autograder system\n",
    "\n",
    "# z_test = X_test[:, sensitive_attribute]\n",
    "\n",
    "# TPR_0 = np.sum((y_pred[z_test == 0] == 1) & (y_test[z_test == 0] == 1)) / np.sum(y_test[z_test == 0] == 1 & (y_test[z_test == 0] == 1))\n",
    "# FPR_0 = np.sum((y_pred[z_test == 0] == 0) & (y_test[z_test == 0] == 1)) / np.sum(y_test[z_test == 0] == 1)\n",
    "\n",
    "# TPR_1 = np.sum((y_pred[z_test == 1] == 1) & (y_test[z_test == 1] == 1)) / np.sum(y_test[z_test == 1] == 1 & (y_test[z_test == 1] == 1))\n",
    "# FPR_1 = np.sum((y_pred[z_test == 1] == 0) & (y_test[z_test == 1] == 1)) / np.sum(y_test[z_test == 1] == 1)\n",
    "\n",
    "# TPR_0, TPR_1, FPR_0, FPR_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cfa79c3",
   "metadata": {},
   "source": [
    "5. Compute PPV_0, PPV_1, NPV_0, and NPV_1 (see \"are fairness goals compatible\" slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "662715ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8306451612903226, 0.8, 0.6486486486486487, 0.7857142857142857)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_test = (X_test[:, sensitive_attribute] == 1)\n",
    "\n",
    "tp_0 = np.sum((y_pred == 1) & (y_test == 1) & (z_test == 0))\n",
    "fn_0 = np.sum((y_pred == 0) & (y_test == 1) & (z_test == 0))\n",
    "fp_0 = np.sum((y_pred == 1) & (y_test == 0) & (z_test == 0))\n",
    "tn_0 = np.sum((y_pred == 0) & (y_test == 0) & (z_test == 0))\n",
    "\n",
    "tp_1 = np.sum((y_pred == 1) & (y_test == 1) & (z_test == 1))\n",
    "fn_1 = np.sum((y_pred == 0) & (y_test == 1) & (z_test == 1))\n",
    "fp_1 = np.sum((y_pred == 1) & (y_test == 0) & (z_test == 1))\n",
    "tn_1 = np.sum((y_pred == 0) & (y_test == 0) & (z_test == 1))\n",
    "\n",
    "\n",
    "PPV_0 = tp_0 / (tp_0 + fp_0)\n",
    "PPV_1 = tp_1 / (tp_1 + fp_1)\n",
    "\n",
    "NPV_0 = tn_0 / (tn_0 + fn_0)\n",
    "NPV_1 = tn_1 / (tn_1 + fn_1)\n",
    "\n",
    "PPV_0, PPV_1, NPV_0, NPV_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4a3d072",
   "metadata": {},
   "source": [
    "6. Implement a \"fairness through unawareness\" classifier, i.e., don\"t use Z in your feature vector. Find the classifier coefficient which undergoes the largest (absolute value) change compared to the classifier with the feature included, and report its new coefficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b504483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.32240286162833937, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new = np.delete(X_train, sensitive_attribute, axis=1)\n",
    "X_test_new = np.delete(X_test, sensitive_attribute, axis=1)\n",
    "\n",
    "new_model = LogisticRegression(C=1, max_iter=1000, random_state=42)\n",
    "new_model.fit(X_train_new, y_train)\n",
    "\n",
    "original_coeff = np.delete(model.coef_.flatten(), sensitive_attribute)\n",
    "new_coeff = new_model.coef_.flatten()\n",
    "\n",
    "coeff_changes = np.abs(original_coeff - new_coeff)\n",
    "biggest_change_idx = np.argmax(coeff_changes)\n",
    "new_coeff = new_coeff[biggest_change_idx]\n",
    "\n",
    "new_coeff, biggest_change_idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a843563",
   "metadata": {},
   "source": [
    "7. Report the demographic parity of the classifier after implementing the above intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0efa6e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.10351966873706"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_new = new_model.predict(X_test_new)\n",
    "\n",
    "z_test = (X_test[:, sensitive_attribute] == 1)\n",
    "\n",
    "positive_pred_z_0 = np.mean(y_pred_new[z_test == 0])\n",
    "positive_pred_z_1 = np.mean(y_pred_new[z_test == 1])\n",
    "\n",
    "new_parity = positive_pred_z_0 / positive_pred_z_1\n",
    "\n",
    "new_parity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcdb181b",
   "metadata": {},
   "source": [
    "8. Report the Generalized False Positive Rate and Generalized False Negative Rate of your original (i.e., not the one with z excluded).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03aa5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code that I got 1.0 / 1.0 for the original autograder system\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# GFPR = np.sum(y_pred[(y_test == 0)]) / np.sum(y_test == 0)\n",
    "# GFNR = np.sum(1 - y_pred[(y_test == 1)]) / np.sum(y_test == 1)\n",
    "\n",
    "# GFPR, GFNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a86c7ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4925768384394524, 0.22529350212751506)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "GFPR = np.sum(y_pred_proba[(y_test == 0), 1]) / np.sum(y_test == 0)\n",
    "GFNR = np.sum(1 - y_pred_proba[(y_test == 1), 1]) / np.sum(y_test == 1)\n",
    "\n",
    "GFPR, GFNR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "968ab77b",
   "metadata": {},
   "source": [
    "9. (harder, 2 marks) Changing the classifier threshold (much as you would to generate an ROC curve) will change the False Positive and False Negative rates for both groups (i.e., FP_0, FP_1, FN_0, FN_1). Implement a \"fairness through unawareness\" classifier like you did in Question 6 but instead use feature 19 (i.e., feat[18]) as the sensitive attribute. Using this classifier, find the (non-trivial) threshold that comes closest to achieving Treatment Equality, and report the corresponding values of FP_0, FP_1, FN_0, and FN_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "356c2098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 2, 22, 2, 0.5655655655655656)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitive_attribute = 18\n",
    "new_model = LogisticRegression(C=1, max_iter=1000, random_state=42)\n",
    "\n",
    "X_train_new = np.delete(X_train, sensitive_attribute, axis=1)\n",
    "X_test_new = np.delete(X_test, sensitive_attribute, axis=1)\n",
    "sensitive_test = X_test[:, sensitive_attribute]\n",
    "\n",
    "new_model.fit(X_train_new, y_train)\n",
    "\n",
    "y_pred_proba = new_model.predict_proba(X_test_new)[:, 1]\n",
    "\n",
    "z_test = (X_test[:, sensitive_attribute] == 1)\n",
    "\n",
    "best_threshold = None\n",
    "min_treatment_diff = float(\"inf\")\n",
    "FP_0, FP_1, FN_0, FN_1 = 0, 0, 0, 0\n",
    "\n",
    "thresholds = np.linspace(0, 1, 1000)\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    fp_0 = np.sum((y_pred == 1) & (y_test == 0) & (z_test == 0))\n",
    "    fn_0 = np.sum((y_pred == 0) & (y_test == 1) & (z_test == 0))\n",
    "\n",
    "    fp_1 = np.sum((y_pred == 1) & (y_test == 0) & (z_test == 1))\n",
    "    fn_1 = np.sum((y_pred == 0) & (y_test == 1) & (z_test == 1))\n",
    "\n",
    "    ratio_0 = fp_0 / fn_0 if fn_0 != 0 else float(\"inf\")\n",
    "    ratio_1 = fp_1 / fn_1 if fn_1 != 0 else float(\"inf\")\n",
    "    \n",
    "    treatment_diff = abs(ratio_0 - ratio_1)\n",
    "    \n",
    "    if treatment_diff < min_treatment_diff:\n",
    "        min_treatment_diff = treatment_diff\n",
    "        best_threshold = threshold\n",
    "        FP_0, FP_1 = fp_0, fp_1\n",
    "        FN_0, FN_1 = fn_0, fn_1\n",
    "\n",
    "FP_0, FP_1, FN_0, FN_1, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c82a6785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': 0.695,\n",
       " 'Q2': [0.7204968944099379, 0.5897435897435898],\n",
       " 'Q3': 1.2014906832298136,\n",
       " 'Q4': [0.8879310344827587, 0.8695652173913043, 0.4666666666666667, 0.3125],\n",
       " 'Q5': [0.8306451612903226, 0.8, 0.6486486486486487, 0.7857142857142857],\n",
       " 'Q6': [19, 0.32240286162833937],\n",
       " 'Q7': 1.10351966873706,\n",
       " 'Q8': [0.4925768384394524, 0.22529350212751506],\n",
       " 'Q9': [22, 2, 22, 2]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = {\n",
    "    \"Q1\": prevalence,           # prevalence\n",
    "    \"Q2\": [prevalence_0, prevalence_1],  # prevalence_0, prevalence_1\n",
    "    \"Q3\": parity,           # parity\n",
    "    \"Q4\": [TPR_0, TPR_1, FPR_0, FPR_1], # TPR_0, TPR_1, FPR_0, FPR_1\n",
    "    \"Q5\": [PPV_0, PPV_1, NPV_0, NPV_1], # PPV_0, PPV_1, NPV_0, NPV_1\n",
    "    \"Q6\": [biggest_change_idx, new_coeff], # feature index, coefficient\n",
    "    \"Q7\": new_parity,           # parity\n",
    "    \"Q8\": [GFPR, GFNR],  # GFPR, GFNR\n",
    "    \"Q9\": [FP_0, FP_1, FN_0, FN_1]  # FP_0, FP_1, FN_0, FN_1\n",
    "}\n",
    "answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
