{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496d83a4-2983-4811-b6ee-a23f0cc4d77e",
   "metadata": {},
   "source": [
    "# Homework 4: Fairness and bias interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57507e53-4e30-4f2d-9e41-94542716052c",
   "metadata": {},
   "source": [
    "## Regression: Download the \"wine quality\" dataset:\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/186/wine+quality\n",
    "\n",
    "## Unzip the file \"wine+quality.zip\" to obtain:\n",
    "\n",
    "- winequality.names\n",
    "- winequality-red.csv\n",
    "- winequality-white.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a3705a-61c6-4c97-be1f-3e738cd583ae",
   "metadata": {},
   "source": [
    "Predifine the answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1be08f-5a17-42c3-a10a-ef49c17d3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362570e5-f232-4acc-bd2f-df447d282cc1",
   "metadata": {},
   "source": [
    "### Implement a  linear regressor using all continuous attributes (i.e., everything except color) to predict the wine quality. Use an 80/20 train/test split. Use sklearn’s `linear_model.LinearRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b557ec0-d6e0-4382-940e-eac85880f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Load datasets\n",
    "winequality_red = pd.read_csv(\"winequality-red.csv\", sep=';')\n",
    "winequality_white = pd.read_csv(\"winequality-white.csv\", sep=';')\n",
    "\n",
    "# Concatenate the datasets\n",
    "wine_data = pd.concat([winequality_red, winequality_white], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Set a random seed and split the train/test subsets\n",
    "random_seed = 42\n",
    "train_data, test_data = train_test_split(wine_data, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "# Display the train and test data\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Train the linear regression model\n",
    "X_train = train_data.drop(columns=['quality'])\n",
    "y_train = train_data['quality']\n",
    "X_test = test_data.drop(columns=['quality'])\n",
    "y_test = test_data['quality']\n",
    "\n",
    "# normalize the dataset\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc26500b-b310-4814-a68c-7bee4a085713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306471a-3204-4dbf-a409-eb33238e96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run linear regression here\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc38b7-b761-42d1-970d-fa0c7d3600b0",
   "metadata": {},
   "source": [
    "1. Report the feature with the largest coefficient value and the corresponding coefficient (not including any offset term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c45f5-ffa8-4c4d-9dc8-2abb844a6f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "coefficients = model.coef_\n",
    "feature_names = X_train.columns\n",
    "largest_coeff_idx = np.argmax(np.abs(coefficients))\n",
    "\n",
    "feature = feature_names[largest_coeff_idx]\n",
    "corresponding_coefficient = coefficients[largest_coeff_idx]\n",
    "print(feature, corresponding_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bfba22a-8d53-4e56-9fd9-06f166cbd7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1'] = [feature, corresponding_coefficient]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776ed66-9575-4f16-afe2-7be06aff6e9f",
   "metadata": {},
   "source": [
    "2. On the first example in the test set, determine which feature has the largest effect and report its effect (see \"Explaining predictions using weight plots & effect plots\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7da36-baf1-4831-9bcd-bdd0a8d67555",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_test_example = X_test_normalized[0]\n",
    "effects = first_test_example * coefficients\n",
    "\n",
    "largest_effect_idx = np.argmax(np.abs(effects))\n",
    "\n",
    "feature = feature_names[largest_effect_idx]\n",
    "corresponding_coefficient = effects[largest_effect_idx]\n",
    "print(feature, corresponding_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "638b206b-2765-488a-8902-7a9571a9d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = [feature, corresponding_coefficient]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c1a03-34ef-4a29-83eb-8e93e1bc36f8",
   "metadata": {},
   "source": [
    "3. (2 marks) Based on the MSE, compute ablations of the model including every feature (other than the offset). Find the most important feature (i.e., such that the ablated model has the highest MSE) and report the value of MSE_ablated - MSE_full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05798b2-b0b4-45ac-93ed-a952701cc52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_normalized)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "mse_ablated = {}\n",
    "for i, feature in enumerate(feature_names):\n",
    "    X_train_ablated = np.delete(X_train_normalized, i, axis=1)\n",
    "    X_test_ablated = np.delete(X_test_normalized, i, axis=1)\n",
    "\n",
    "    model_ablated = LinearRegression()\n",
    "    model_ablated.fit(X_train_ablated, y_train)\n",
    "\n",
    "    y_pred_ablated = model_ablated.predict(X_test_ablated)\n",
    "    mse_ablated[feature] = mean_squared_error(y_test, y_pred_ablated)\n",
    "\n",
    "most_important_feature = max(mse_ablated, key=lambda k: mse_ablated[k] - mse)\n",
    "mse_diff = mse_ablated[most_important_feature] - mse\n",
    "print(most_important_feature, mse_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "618a73ab-3184-4f42-b3cd-a34fb43d1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3'] = [most_important_feature, mse_diff]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50bc1c-0e2c-44ba-bb02-be6dea13c6a2",
   "metadata": {},
   "source": [
    "4. (2 marks) Implement a full backward selection pipeline and report the sequence of MSE values for each model as a list (of increasing MSEs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa208d-7705-4979-94e2-9bf22b780ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_features = list(feature_names)\n",
    "print(len(remaining_features))\n",
    "X_train_current = X_train_normalized.copy()\n",
    "X_test_current = X_test_normalized.copy()\n",
    "\n",
    "mse_values = []\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_current, y_train)\n",
    "y_pred_full = model.predict(X_test_current)\n",
    "mse_values.append(mean_squared_error(y_test, y_pred_full))\n",
    "\n",
    "while len(remaining_features) > 1:\n",
    "    mse_ablated = {}\n",
    "\n",
    "    for i, feature in enumerate(remaining_features):\n",
    "        X_train_ablated = np.delete(X_train_current, i, axis=1)\n",
    "        X_test_ablated = np.delete(X_test_current, i, axis=1)\n",
    "\n",
    "        model_ablated = LinearRegression()\n",
    "        model_ablated.fit(X_train_ablated, y_train)\n",
    "        y_pred_ablated = model_ablated.predict(X_test_ablated)\n",
    "        mse_ablated[feature] = mean_squared_error(y_test, y_pred_ablated)\n",
    "\n",
    "    least_important_feature = min(mse_ablated, key=lambda k: mse_ablated[k])\n",
    "    mse_values.append(mse_ablated[least_important_feature])\n",
    "\n",
    "    remove_idx = remaining_features.index(least_important_feature)\n",
    "    X_train_current = np.delete(X_train_current, remove_idx, axis=1)\n",
    "    X_test_current = np.delete(X_test_current, remove_idx, axis=1)\n",
    "    remaining_features.remove(least_important_feature)\n",
    "\n",
    "mse_list = sorted(mse_values)\n",
    "mse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ad75e6c-4798-4033-b918-8a2f8f98df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4'] = mse_list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc8067-aadc-4e21-869d-5c7a627d4949",
   "metadata": {},
   "source": [
    "5. (2 marks) Change your model to use an l1 regularizer. Increasing the regularization strength will cause variables to gradually be removed (coefficient reduced to zero) from the model. Which is the first and the last variable to be eliminated via this process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160f4a3-0ab8-4db1-9b42-47ecce3a5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha_values = np.logspace(-4, 1, 50)\n",
    "feature_names = np.array(feature_names)\n",
    "\n",
    "coef_history = np.zeros((len(alpha_values), len(feature_names)))\n",
    "\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    lasso = Lasso(alpha=alpha, max_iter=5000)\n",
    "    lasso.fit(X_train_normalized, y_train)\n",
    "    coef_history[i] = lasso.coef_\n",
    "\n",
    "first_eliminated_index = np.where(coef_history == 0)[1][0]\n",
    "last_eliminated_index = np.where(coef_history == 0)[1][-1]\n",
    "\n",
    "first_feature = feature_names[first_eliminated_index]\n",
    "last_feature = feature_names[last_eliminated_index]\n",
    "print(first_feature, last_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76b7fff2-cdc9-4f5a-939f-7fd916aee222",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = [first_feature, last_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d11bf8-8f0c-40d5-82e5-9a543f202aa8",
   "metadata": {},
   "source": [
    "### Implement a classifier to predict the wine color (red / white), again using an 80/20 train/test split, and including only continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2733fcb-5c2a-4d58-acad-d3bfa8ecbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load datasets\n",
    "winequality_red = pd.read_csv(\"winequality-red.csv\", sep=';')\n",
    "winequality_white = pd.read_csv(\"winequality-white.csv\", sep=';')\n",
    "\n",
    "# Add a column to distinguish red and white wines\n",
    "winequality_red['type'] = 0  # Red wine (encoded as 0)\n",
    "winequality_white['type'] = 1  # White wine (encoded as 1)\n",
    "\n",
    "# Concatenate the datasets\n",
    "wine_data = pd.concat([winequality_red, winequality_white], axis=0)\n",
    "\n",
    "# Separate features (and drop \"quality\" to get continuous variables) and target\n",
    "X = wine_data.drop(columns=['quality', 'type'])  # Drop the target column\n",
    "y = wine_data['type']  # Target column (wine type)\n",
    "\n",
    "# Perform train/test split\n",
    "random_seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "# Display shapes of the resulting splits\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a0ada-2909-4b27-940d-4c808d725809",
   "metadata": {},
   "source": [
    "6. Report the odds ratio associated with the first sample in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe32c1f-b351-42df-8d71-71570f2c3bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X=X_train, y=y_train)\n",
    "\n",
    "first_sample = X_test.iloc[0].values.reshape(1, -1)\n",
    "prob_white = model.predict_proba(first_sample)[0, 1]\n",
    "odds_ratio = prob_white / (1 - prob_white)\n",
    "print(odds_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9a61b47-6c29-45f5-96ce-66339db779e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6'] = odds_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c91810-ec59-4053-a0b9-79309f88c0ff",
   "metadata": {},
   "source": [
    "7. Find the 20 nearest neighbors (in the training set) to the first datapoint in the test set, based on the l2 distance. Train a classifier using only those 20 points, and report the largest value of e^theta_j (see “odds ratio” slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28b9cc41-5178-46fb-a203-4e08f69f3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26291b68-8309-4ecf-b6ec-cff0e8137bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
